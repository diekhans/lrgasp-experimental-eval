* hub dir https://hgwdev.gi.ucsc.edu/~markd/lrgasp/experimental-eval/hub/hg38/dir/
* IGV tracks
https://hgwdev.gi.ucsc.edu/~markd/gencode/lrgasp/experimental-eval/hub/hg38/targets.bb
https://hgwdev.gi.ucsc.edu/~markd/gencode/lrgasp/experimental-eval/hub/hg38/pb+ont-rna-wtc11.sim=0.98.genome.bam
https://hgwdev.gi.ucsc.edu/~markd/gencode/lrgasp/experimental-eval/hub/hg38/pb+ont-rna-wtc11.sim=0.95.genome.bam
https://hgwdev.gi.ucsc.edu/~markd/gencode/lrgasp/experimental-eval/hub/hg38/pb+ont-rna-wtc11.sim=0.90.genome.bam

* squanti data
** Francisco J. Pardo-Palacios
I think I already have the data you need uploaded
to synapse I calculated the median CPM per Unique Intron Chain using all the
classification files in a given sample (i.e. WTC11). In those tsv files, the
FL column stands for the number of reads that the submitters used to build
their model according to their read2transcript file, and the UIC name
corresponds to the column LRGASP_ID . And that's all I used The evaluation
results can be found in the LRGASP synapse --> Files --> results -->
Challenge1 For each tool/submitter, I uploaded a compressed folder that
includes the evaluation for all the samples, platforms and library preps
submitted (files named *classification.txt ). These files were given to the
submitters when we finished the evaluation, so they should be already public
to some extent

* BED with Squanti stats
http://conesalab.org/LRGASP/LRGASP_hub/hg38/Human_samples/WTC11_consolidated.bigBed

* Gloria spread sheet
https://docs.google.com/spreadsheets/d/1nW8W_xtfvKE1sSDBNRsZ3K6HWEtxb4jWwZtPh5hNMVA/edit#gid=0

* JuJu spread sheet
https://docs.google.com/spreadsheets/d/1Y3KhZoezR8Ti5mauYXQFxuwE04xmEgPPyxoJoRxuYuA/edit#gid=210377221


* wtc11: gather amplicon and transcript lengths, and squanti data
cd tmp/wtc11

# wtc11-trans_len.tsv
bigBedToBed ../../../primers/primer-design/data/hg38/WTC11_consolidated.bigBed stdout | cut -f 1-12 >WTC11_consolidated.bed
(echo "transcript${tab}transcript_len" ; (twoBitToFa -bed=WTC11_consolidated.bed  /hive/data/genomes/hg38/hg38.2bit  stdout | faSize -detailed /dev/stdin)) >wtc11-trans_len.tsv

# wtc11.lengths.tsv
tmlr filter '$pri==1' ../../../primers/primer-design/hub/hg38/juju_designs.hg38.isoforms.tsv | tmlr join -j transcript_id -l transcript -r transcript_id -f wtc11-trans_len.tsv | tmlr rename target_id,target,transcript_id,transcript | tmlr cut -o -f target,transcript,transcript_len,amplicon_len  >../../evaluation/wtc11.lengths.tsv

# get squanti stats
bigBedToBed -header http://conesalab.org/LRGASP/LRGASP_hub/hg38/Human_samples/WTC11_consolidated.bigBed stdout | sed 's/^#//' > WTC11_consolidated.bed.tsv


# wtc11-lengths.rows.tsv (for spreadsheet merge)
../../bin/spreadSheetOrder --rowsToAddPerGroup=2 wtc11-support.tsv wtc11.isoforms.tsv /dev/stdout | tmlr cut -o -f target,transcript,trans_len,amplicon_len >wtc11-lengths.rows.tsv

* gencode: gather amplicon and transcript lengths
cd tmp/gencode

# gencode-trans_len.tsv
for bb in  ../../../primers/primer-design/data/hg38/human_GENCODE_tmerge_transcripts.bb ../../../primers/primer-design/data/hg38/non_redundant_*.bb ; do bigBedToBed $bb stdout | cut -f 1-12 ; done > gencode-combined.bed 
(echo "transcript${tab}transcript_len" ; (twoBitToFa -bed=gencode-combined.bed /hive/data/genomes/hg38/hg38.2bit stdout) | faSize -detailed stdin) >gencode-trans_len.tsv

# gencode.lengths.tsv
tmlr filter '$pri==1' ../../../primers/primer-design/hub/hg38/juju_designs.hg38.isoforms.tsv | tmlr join -j transcript_id -l transcript -r transcript_id -f gencode-trans_len.tsv | tmlr rename target_id,target,transcript_id,transcript | tmlr cut -o -f target,transcript,transcript_len,amplicon_len  >../../evaluation/gencode.lengths.tsv


# gencode-lengths.rows.tsv (for spreadsheet merge)
../../bin/spreadSheetOrder --rowsToAddPerGroup=2 gencode-support.tsv gencode.isoforms.tsv /dev/stdout | tmlr cut -o -f gene,target,transcript,trans_len,amplicon_len >gencode-lengths.rows.tsv


 
* stats for paper
../bin/paperEvalStats wtc11-support.summary.tsv /dev/stdout
GENCODE-known, N=XX:
GENCODE-novel, N=XX:,
GENCODE-suspect, N=XX:
GENCODE-known validation rate, XX%
GENCODE-known that failed to validate n+XX
GENCODE-novel   validation rate, XX %
GENCODE-suspect validation rate of XX,
GENCODE-suspect “validated” XX

novel isoforms count:
novel isoforms detected in N pipelines (different customs)
novel isoforms in less name N pipelines

validation rate for isoforms unique 
we find XX validation rate for isoforms that were not reproducible across pipelines:

In order to maintain a fair comparison, we limited selection of targets to <XX kb,
targets with at least an average TPM of XX

An isoform, X, which was detected in XX of XX ONT pipelines but only XX of XX PacBio pipelines

including cases of a single long read (Figure XX).

We found that [Mark add an example here] deemed validated revealed that an isoform for gene XX had an
incorrect alignment.

* get fine-grained pipeline information
cd work
find ../squanti/results/Challenge1 -name models_classification.txt | fgrep WTC11 >filt.sh
# covert to commands like
../bin/joinSquantiTransId ../evaluation/wtc11.id_ujc.tsv ../squanti/results/Challenge1/Mandalorion_challenge1_evaluation/Mandalorion_cDNA_PacBio_LRGASP_submission/WTC11_PacBioOnly/models_classification.txt joined/Challenge1/Mandalorion_challenge1_evaluation/Mandalorion_cDNA_PacBio_LRGASP_submission/WTC11_PacBioOnly/models_classification.txt
nice parallel -j 32 <filt.sh >&log
tmlr cat $(find joined/Challenge1/ -type f) >wtc11.squnati-transcripts.tsv

* paper stats

cd evaluation/wtc11
../../bin/paperEvalStats wtc11.evaluation.tsv wtc11.stats.tsv
../../bin/squantiSummary ../../../paper/paper-sync/Stage_2_submission/Challenge1_SuppTables/Presence-absence\ matrices\ by\ UJC/WTC11.pa.csv wtc11.evaluation.tsv wtc11.id_ujc.tsv wtc11.squanti-summary.tsv wtc11.squanti-stats.tsv

cd evaluation/gencode
../../bin/paperEvalStats gencode.evaluation.tsv  gencode.stats.tsv

