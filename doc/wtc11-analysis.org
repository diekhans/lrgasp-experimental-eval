* hub dir https://hgwdev.gi.ucsc.edu/~markd/lrgasp/experimental-eval/hub/hg38/dir/
* IGV tracks
https://hgwdev.gi.ucsc.edu/~markd/gencode/lrgasp/experimental-eval/hub/hg38/targets.bb
https://hgwdev.gi.ucsc.edu/~markd/gencode/lrgasp/experimental-eval/hub/hg38/pb+ont-rna-wtc11.sim=0.98.genome.bam
https://hgwdev.gi.ucsc.edu/~markd/gencode/lrgasp/experimental-eval/hub/hg38/pb+ont-rna-wtc11.sim=0.95.genome.bam
https://hgwdev.gi.ucsc.edu/~markd/gencode/lrgasp/experimental-eval/hub/hg38/pb+ont-rna-wtc11.sim=0.90.genome.bam

* squanti data
** Francisco J. Pardo-Palacios
I think I already have the data you need uploaded
to synapse I calculated the median CPM per Unique Intron Chain using all the
classification files in a given sample (i.e. WTC11). In those tsv files, the
FL column stands for the number of reads that the submitters used to build
their model according to their read2transcript file, and the UIC name
corresponds to the column LRGASP_ID . And that's all I used The evaluation
results can be found in the LRGASP synapse --> Files --> results -->
Challenge1 For each tool/submitter, I uploaded a compressed folder that
includes the evaluation for all the samples, platforms and library preps
submitted (files named *classification.txt ). These files were given to the
submitters when we finished the evaluation, so they should be already public
to some extent

* BED with Squanti stats
http://conesalab.org/LRGASP/LRGASP_hub/hg38/Human_samples/WTC11_consolidated.bigBed

* Gloria spread sheet
https://docs.google.com/spreadsheets/d/1nW8W_xtfvKE1sSDBNRsZ3K6HWEtxb4jWwZtPh5hNMVA/edit#gid=0

* JuJu spread sheet
https://docs.google.com/spreadsheets/d/1Y3KhZoezR8Ti5mauYXQFxuwE04xmEgPPyxoJoRxuYuA/edit#gid=210377221


* wtc11: gather amplicon and transcript lengths, and squanti data
cd tmp/wtc11

# wtc11-trans_len.tsv
bigBedToBed ../../../primers/primer-design/data/hg38/WTC11_consolidated.bigBed stdout | cut -f 1-12 >WTC11_consolidated.bed
(echo "transcript${tab}transcript_len" ; (twoBitToFa -bed=WTC11_consolidated.bed  /hive/data/genomes/hg38/hg38.2bit  stdout | faSize -detailed /dev/stdin)) >wtc11-trans_len.tsv

# wtc11.lengths.tsv
tmlr filter '$pri==1' ../../../primers/primer-design/hub/hg38/juju_designs.hg38.isoforms.tsv | tmlr join -j transcript_id -l transcript -r transcript_id -f wtc11-trans_len.tsv | tmlr rename target_id,target,transcript_id,transcript | tmlr cut -o -f target,transcript,transcript_len,amplicon_len  >../../evaluation/wtc11.lengths.tsv

# get squanti stats
bigBedToBed -header http://conesalab.org/LRGASP/LRGASP_hub/hg38/Human_samples/WTC11_consolidated.bigBed stdout | sed 's/^#//' > WTC11_consolidated.bed.tsv


# wtc11-lengths.rows.tsv (for spreadsheet merge)
../../bin/spreadSheetOrder --rowsToAddPerGroup=2 wtc11-support.tsv wtc11.isoforms.tsv /dev/stdout | tmlr cut -o -f target,transcript,trans_len,amplicon_len >wtc11-lengths.rows.tsv

* gencode: gather amplicon and transcript lengths
cd tmp/gencode

# gencode-trans_len.tsv
for bb in  ../../../primers/primer-design/data/hg38/human_GENCODE_tmerge_transcripts.bb ../../../primers/primer-design/data/hg38/non_redundant_*.bb ; do bigBedToBed $bb stdout | cut -f 1-12 ; done > gencode-combined.bed 
(echo "transcript${tab}transcript_len" ; (twoBitToFa -bed=gencode-combined.bed /hive/data/genomes/hg38/hg38.2bit stdout) | faSize -detailed stdin) >gencode-trans_len.tsv

# gencode.lengths.tsv
tmlr filter '$pri==1' ../../../primers/primer-design/hub/hg38/juju_designs.hg38.isoforms.tsv | tmlr join -j transcript_id -l transcript -r transcript_id -f gencode-trans_len.tsv | tmlr rename target_id,target,transcript_id,transcript | tmlr cut -o -f target,transcript,transcript_len,amplicon_len  >../../evaluation/gencode.lengths.tsv


# gencode-lengths.rows.tsv (for spreadsheet merge)
../../bin/spreadSheetOrder --rowsToAddPerGroup=2 gencode-support.tsv gencode.isoforms.tsv /dev/stdout | tmlr cut -o -f gene,target,transcript,trans_len,amplicon_len >gencode-lengths.rows.tsv


 
* stats for paper
../bin/paperEvalStats wtc11-support.summary.tsv /dev/stdout
GENCODE-known, N=XX:
GENCODE-novel, N=XX:,
GENCODE-suspect, N=XX:
GENCODE-known validation rate, XX%
GENCODE-known that failed to validate n+XX
GENCODE-novel   validation rate, XX %
GENCODE-suspect validation rate of XX,
GENCODE-suspect “validated” XX

novel isoforms count:
novel isoforms detected in N pipelines (different customs)
novel isoforms in less name N pipelines

validation rate for isoforms unique 
we find XX validation rate for isoforms that were not reproducible across pipelines:

In order to maintain a fair comparison, we limited selection of targets to <XX kb,
targets with at least an average TPM of XX

An isoform, X, which was detected in XX of XX ONT pipelines but only XX of XX PacBio pipelines

including cases of a single long read (Figure XX).

We found that [Mark add an example here] deemed validated revealed that an isoform for gene XX had an
incorrect alignment.

* get fine-grained pipeline information
cd work
find ../squanti/results/Challenge1 -name models_classification.txt | fgrep WTC11 >filt.sh
# covert to commands like
../bin/joinSquantiTransId ../evaluation/wtc11.id_ujc.tsv ../squanti/results/Challenge1/Mandalorion_challenge1_evaluation/Mandalorion_cDNA_PacBio_LRGASP_submission/WTC11_PacBioOnly/models_classification.txt joined/Challenge1/Mandalorion_challenge1_evaluation/Mandalorion_cDNA_PacBio_LRGASP_submission/WTC11_PacBioOnly/models_classification.txt
nice parallel -j 32 <filt.sh >&log
tmlr cat $(find joined/Challenge1/ -type f) >wtc11.squnati-transcripts.tsv

* paper stats

cd evaluation
../bin/paperEvalStats  --squantiTsv=wtc11.squanti-consolidate.tsv wtc11.evaluation.tsv wtc11.stats.tsv
../bin/paperEvalStats gencode.evaluation.tsv  gencode.stats.tsv


* wtc11
pipelines:     33   half: 16.5
novelIsoforms  64   half: 32

novelPipelineCnt_ge_1  64   novelPipelineCnt_lt_1   0
novelPipelineCnt_ge_2  48   novelPipelineCnt_lt_2  16
novelPipelineCnt_ge_3  46   novelPipelineCnt_lt_3  18
novelPipelineCnt_ge_4  46   novelPipelineCnt_lt_4  18
novelPipelineCnt_ge_5  46   novelPipelineCnt_lt_5  18
novelPipelineCnt_ge_6  45   novelPipelineCnt_lt_6  19
novelPipelineCnt_ge_7  43   novelPipelineCnt_lt_7  21
novelPipelineCnt_ge_8  40   novelPipelineCnt_lt_8  24
novelPipelineCnt_ge_9  39   novelPipelineCnt_lt_9  25
novelPipelineCnt_ge_10 37   novelPipelineCnt_lt_10 27
novelPipelineCnt_ge_11 36   novelPipelineCnt_lt_11 28
novelPipelineCnt_ge_12 33   novelPipelineCnt_lt_12 31
novelPipelineCnt_ge_13 33   novelPipelineCnt_lt_13 31

novelPipelineCnt_ge_14 32   novelPipelineCnt_lt_14 32   0.42 of pipelines detect 1/2 of novel transcritps

novelPipelineCnt_ge_15 30   novelPipelineCnt_lt_15 34
novelPipelineCnt_ge_16 30   novelPipelineCnt_lt_16 34
novelPipelineCnt_ge_17 29   novelPipelineCnt_lt_17 35
novelPipelineCnt_ge_18 27   novelPipelineCnt_lt_18 37
novelPipelineCnt_ge_19 24   novelPipelineCnt_lt_19 40
novelPipelineCnt_ge_20 24   novelPipelineCnt_lt_20 40
novelPipelineCnt_ge_21 23   novelPipelineCnt_lt_21 41
novelPipelineCnt_ge_22 23   novelPipelineCnt_lt_22 41
novelPipelineCnt_ge_23 22   novelPipelineCnt_lt_23 42
novelPipelineCnt_ge_24 20   novelPipelineCnt_lt_24 44
novelPipelineCnt_ge_25 16   novelPipelineCnt_lt_25 48
novelPipelineCnt_ge_26 15   novelPipelineCnt_lt_26 49
novelPipelineCnt_ge_27 13   novelPipelineCnt_lt_27 51
novelPipelineCnt_ge_28 10   novelPipelineCnt_lt_28 54
novelPipelineCnt_ge_29  8   novelPipelineCnt_lt_29 56
novelPipelineCnt_ge_30  7   novelPipelineCnt_lt_30 57
novelPipelineCnt_ge_31  6   novelPipelineCnt_lt_31 58
novelPipelineCnt_ge_32  5   novelPipelineCnt_lt_32 59
novelPipelineCnt_ge_33  2   novelPipelineCnt_lt_33 62


